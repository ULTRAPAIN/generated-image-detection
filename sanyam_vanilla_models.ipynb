{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla models test\n",
    "\n",
    "- Artificial Neural Networks\n",
    "- Decision Tree Classifier\n",
    "- Random Forest\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "valid = pd.read_csv(\"data/valid.csv\")\n",
    "test  = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_n_images(df,no_imgs=10000,shuffle=False):\n",
    "    each_label_images = int(no_imgs / 2)\n",
    "    subset_label_0 = df[df['label'] == 0].sample(n=each_label_images, random_state=69)\n",
    "    subset_label_1 = df[df['label'] == 1].sample(n=each_label_images, random_state=69)\n",
    "    final_subset = pd.concat([subset_label_0, subset_label_1]) \n",
    "    if shuffle == True:\n",
    "        final_subset = final_subset.sample(frac=1, random_state=69).reset_index(drop=True)\n",
    "    return final_subset\n",
    "\n",
    "train_subset = random_n_images(train,no_imgs=10000,shuffle=True)\n",
    "valid_subset = random_n_images(valid,no_imgs=2000,shuffle=False)\n",
    "test_subset = random_n_images(test,no_imgs=2000,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "image_height = 256\n",
    "image_width = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfds = tf.data.Dataset.from_tensor_slices((train_subset['path'].values,train_subset['label'].values))\n",
    "valid_tfds = tf.data.Dataset.from_tensor_slices((valid_subset['path'].values,valid_subset['label'].values))\n",
    "test_tfds = tf.data.Dataset.from_tensor_slices((test_subset['path'].values,test_subset['label'].values))\n",
    "\n",
    "def read_image(image_file, label):\n",
    "    image = tf.io.read_file(\"data/real_vs_fake/real-vs-fake/\" + image_file)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "def augument(image, label):\n",
    "    return image, label\n",
    "\n",
    "train_tfds = train_tfds.map(read_image).map(augument).batch(BATCH_SIZE)\n",
    "valid_tfds = valid_tfds.map(read_image).map(augument).batch(1).prefetch(1)\n",
    "test_tfds = test_tfds.map(read_image).map(augument).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_tfds.take(1):  # Adjust the number of batches as needed\n",
    "    for i in range(2):\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(images[i, :, :, :].numpy())\n",
    "        plt.title(f\"Label: {labels[i].numpy()}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "resnet_clf = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = ModelCheckpoint('/models/training_1'\n",
    "                                ,save_weights_only=True\n",
    "                                ,monitor='val_accuracy'\n",
    "                                ,save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss'\n",
    "                                        ,patience=5)\n",
    "\n",
    "resnet_clf.compile(optimizer=Adam()\n",
    "                   ,loss='binary_crossentropy'\n",
    "                   ,metrics=['accuracy'])\n",
    "# resnet_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "resnet_clf_history = resnet_clf.fit(\n",
    "    train_tfds,\n",
    "    epochs=100,\n",
    "    validation_data=valid_tfds,\n",
    "    callbacks=[save_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results\n",
    "training_loss = resnet_clf_history.history['loss']\n",
    "validation_loss = resnet_clf_history.history['val_loss']\n",
    "training_accuracy = resnet_clf_history.history['accuracy']\n",
    "validation_accuracy = resnet_clf_history.history['val_accuracy']\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plotting Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, training_loss, label='Training loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, training_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set\n",
    "resnet_clf.evaluate(test_tfds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "from datetime import datetime\n",
    "filename = f'models/resnet_clf_{datetime.now().timestamp()}.h5'\n",
    "tf.keras.models.save_model(resnet_clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real image\n",
    "img_path = \"manual_test_images/Real.jpg\"\n",
    "img = image.load_img(img_path, target_size=(256, 256))\n",
    "plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 256, 256, 3))\n",
    "\n",
    "# Assuming resnet_clf_history is your ResNet model\n",
    "prediction = resnet_clf.predict(img_array)\n",
    "\n",
    "# Process the prediction as needed\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake image(hard)\n",
    "img_path = \"manual_test_images/Hard.jpeg\"\n",
    "img = image.load_img(img_path, target_size=(256, 256))\n",
    "plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 256, 256, 3))\n",
    "\n",
    "# Assuming resnet_clf_history is your ResNet model\n",
    "prediction = resnet_clf.predict(img_array)\n",
    "\n",
    "# Process the prediction as needed\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake image(hard)\n",
    "img_path = \"manual_test_images/Hard.jpeg\"\n",
    "img = image.load_img(img_path, target_size=(256, 256))\n",
    "plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 256, 256, 3))\n",
    "\n",
    "# Assuming resnet_clf_history is your ResNet model\n",
    "prediction = resnet_clf.predict(img_array)\n",
    "\n",
    "# Process the prediction as needed\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake image(easy)\n",
    "img_path = \"manual_test_images/Easy.jpg\"\n",
    "img = image.load_img(img_path, target_size=(256, 256))\n",
    "plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 256, 256, 3))\n",
    "\n",
    "# Assuming resnet_clf_history is your ResNet model\n",
    "prediction = resnet_clf.predict(img_array)\n",
    "\n",
    "# Process the prediction as needed\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the best model so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 152s 76ms/step - loss: 0.2050 - accuracy: 0.9250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20501020550727844, 0.925000011920929]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"models/resnet_clf_1703610668.109097.h5\")\n",
    "model.evaluate(test_tfds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
