{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla models test\n",
    "\n",
    "- Artificial Neural Networks\n",
    "- Decision Tree Classifier\n",
    "- Random Forest\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9707045019894422791\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1745302324\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5442142398385891752\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/real_vs_fake/real-vs-fake/train'\n",
    "valid_dir = 'data/real_vs_fake/real-vs-fake/valid'\n",
    "test_dir =  'data/real_vs_fake/real-vs-fake/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    data_format='channels_last',\n",
    "    rotation_range = 45,\n",
    "    zoom_range = (.95,1.05),\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "valid_test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "image_height = 128\n",
    "image_width = 128\n",
    "\n",
    "# Create the generators for training, validation, and test data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    shuffle=True,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',     # Set the class mode to 'binary' since have two classes\n",
    ")\n",
    "\n",
    "valid_generator = valid_test_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = valid_test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fake images have label 0 and real images have label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fake', 'real']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = list(train_generator.class_indices.keys())\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib_inline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m train_generator\u001b[38;5;241m.\u001b[39mnext()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display the images along with their labels\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplots\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(axes\u001b[38;5;241m.\u001b[39mflat):\n\u001b[0;32m      8\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(images[i])\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py:1598\u001b[0m, in \u001b[0;36msubplots\u001b[1;34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubplots\u001b[39m(\n\u001b[0;32m   1445\u001b[0m     nrows: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, ncols: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1446\u001b[0m     sharex: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfig_kw\n\u001b[0;32m   1454\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Figure, Any]:\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    Create a figure and a set of subplots.\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m \n\u001b[0;32m   1597\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1598\u001b[0m     fig \u001b[38;5;241m=\u001b[39m figure(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfig_kw)\n\u001b[0;32m   1599\u001b[0m     axs \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39mnrows, ncols\u001b[38;5;241m=\u001b[39mncols, sharex\u001b[38;5;241m=\u001b[39msharex, sharey\u001b[38;5;241m=\u001b[39msharey,\n\u001b[0;32m   1600\u001b[0m                        squeeze\u001b[38;5;241m=\u001b[39msqueeze, subplot_kw\u001b[38;5;241m=\u001b[39msubplot_kw,\n\u001b[0;32m   1601\u001b[0m                        gridspec_kw\u001b[38;5;241m=\u001b[39mgridspec_kw, height_ratios\u001b[38;5;241m=\u001b[39mheight_ratios,\n\u001b[0;32m   1602\u001b[0m                        width_ratios\u001b[38;5;241m=\u001b[39mwidth_ratios)\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fig, axs\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py:934\u001b[0m, in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(allnums) \u001b[38;5;241m==\u001b[39m max_open_warning \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    925\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_open_warning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m figures have been opened. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    927\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigures created through the pyplot interface \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `matplotlib.pyplot.close()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    932\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m--> 934\u001b[0m manager \u001b[38;5;241m=\u001b[39m new_figure_manager(\n\u001b[0;32m    935\u001b[0m     num, figsize\u001b[38;5;241m=\u001b[39mfigsize, dpi\u001b[38;5;241m=\u001b[39mdpi,\n\u001b[0;32m    936\u001b[0m     facecolor\u001b[38;5;241m=\u001b[39mfacecolor, edgecolor\u001b[38;5;241m=\u001b[39medgecolor, frameon\u001b[38;5;241m=\u001b[39mframeon,\n\u001b[0;32m    937\u001b[0m     FigureClass\u001b[38;5;241m=\u001b[39mFigureClass, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    938\u001b[0m fig \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fig_label:\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py:464\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_figure_manager\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m     \u001b[43m_warn_if_gui_out_of_main_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mnew_figure_manager(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py:441\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[1;34m()\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_warn_if_gui_out_of_main_thread\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     canvas_class \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtype\u001b[39m[FigureCanvasBase], \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mFigureCanvas)\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m canvas_class\u001b[38;5;241m.\u001b[39mrequired_interactive_framework:\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(threading, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_native_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    444\u001b[0m             \u001b[38;5;66;03m# This compares native thread ids because even if Python-level\u001b[39;00m\n\u001b[0;32m    445\u001b[0m             \u001b[38;5;66;03m# Thread objects match, the underlying OS thread (which is what\u001b[39;00m\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;66;03m# really matters) may be different on Python implementations with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# green threads.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py:280\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[1;34m()\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03mEnsure that a backend is selected and return it.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03mThis is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _backend_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# Use rcParams._get(\"backend\") to avoid going through the fallback\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# logic (which will (re)import pyplot and then call switch_backend if\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# we need to resolve the auto sentinel)\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m     \u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;28mtype\u001b[39m[matplotlib\u001b[38;5;241m.\u001b[39mbackend_bases\u001b[38;5;241m.\u001b[39m_Backend], _backend_mod)\n",
      "File \u001b[1;32mc:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py:342\u001b[0m, in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# have to escape the switch on access logic\u001b[39;00m\n\u001b[0;32m    340\u001b[0m old_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(rcParams, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 342\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_module_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m canvas_class \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mFigureCanvas\n\u001b[0;32m    345\u001b[0m required_framework \u001b[38;5;241m=\u001b[39m canvas_class\u001b[38;5;241m.\u001b[39mrequired_interactive_framework\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib_inline'"
     ]
    }
   ],
   "source": [
    "# Checking some images\n",
    "images, labels = train_generator.next()\n",
    "\n",
    "# Display the images along with their labels\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis('off')\n",
    "\n",
    "    if int(labels[i]) == 0:\n",
    "        title_color = 'red'\n",
    "    else:\n",
    "        title_color = 'green'\n",
    "\n",
    "    ax.set_title(classes[int(labels[i])], color=title_color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "cnn = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "cnn.add(Conv2D(16, (5, 5), activation='relu',padding='same',strides=(1,1),input_shape=(image_height, image_width, 3)))\n",
    "cnn.add(Conv2D(32, (3, 3), activation='relu',padding='same',strides=(2,2)))\n",
    "cnn.add(Conv2D(64, (3, 3), activation='relu',padding='same',strides=(1,1)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(2,2)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(1,1)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(2,2)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(1,1)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(2,2)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(1,1)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(2,2)))\n",
    "cnn.add(Conv2D(128, (3, 3), activation='relu',padding='same',strides=(1,1)))\n",
    "cnn.add(Conv2D(128, (4, 4), activation='relu',padding='valid',strides=(1,1)))\n",
    "\n",
    "# # Flatten layer to feed into dense Dense\n",
    "cnn.add(Flatten())\n",
    "\n",
    "# # Dense \n",
    "# cnn.add(Dropout(0.5))\n",
    "# cnn.add(Dense(128, activation='relu'))\n",
    "# cnn.add(Dropout(0.5))\n",
    "# cnn.add(Dense(64, activation='relu'))\n",
    "# cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(1, activation='sigmoid'))  # Adjust the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 16)      1216      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 1, 1, 128)         262272    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,393,697\n",
      "Trainable params: 1,393,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer='adam'\n",
    "            ,loss='binary_crossentropy'\n",
    "            ,metrics=['acc',f1_m,precision_m, recall_m])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3125/3125 [==============================] - 1450s 462ms/step - loss: 0.6932 - acc: 0.4984 - f1_m: 0.3783 - precision_m: 0.2855 - recall_m: 0.5725 - val_loss: 0.6932 - val_acc: 0.5000 - val_f1_m: 0.6621 - val_precision_m: 0.5000 - val_recall_m: 1.0000\n",
      "Epoch 2/10\n",
      "3125/3125 [==============================] - 922s 295ms/step - loss: 0.6932 - acc: 0.5007 - f1_m: 0.3680 - precision_m: 0.2781 - recall_m: 0.5555 - val_loss: 0.6932 - val_acc: 0.5000 - val_f1_m: 0.6621 - val_precision_m: 0.5000 - val_recall_m: 1.0000\n",
      "Epoch 3/10\n",
      "3125/3125 [==============================] - 740s 237ms/step - loss: 0.6932 - acc: 0.4963 - f1_m: 0.3678 - precision_m: 0.2773 - recall_m: 0.5584 - val_loss: 0.6932 - val_acc: 0.5000 - val_f1_m: 0.6626 - val_precision_m: 0.5000 - val_recall_m: 1.0000\n",
      "Epoch 4/10\n",
      "3125/3125 [==============================] - 345s 110ms/step - loss: 0.6932 - acc: 0.4986 - f1_m: 0.3489 - precision_m: 0.2633 - recall_m: 0.5280 - val_loss: 0.6931 - val_acc: 0.5000 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/10\n",
      "3125/3125 [==============================] - 347s 111ms/step - loss: 0.6932 - acc: 0.4999 - f1_m: 0.3095 - precision_m: 0.2339 - recall_m: 0.4678 - val_loss: 0.6932 - val_acc: 0.5000 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/10\n",
      "3125/3125 [==============================] - 340s 109ms/step - loss: 0.6932 - acc: 0.5002 - f1_m: 0.3431 - precision_m: 0.2592 - recall_m: 0.5181 - val_loss: 0.6932 - val_acc: 0.5000 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/10\n",
      "3125/3125 [==============================] - 340s 109ms/step - loss: 0.6932 - acc: 0.4999 - f1_m: 0.3885 - precision_m: 0.2934 - recall_m: 0.5869 - val_loss: 0.6931 - val_acc: 0.5000 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/10\n",
      "3125/3125 [==============================] - 334s 107ms/step - loss: 0.6932 - acc: 0.5018 - f1_m: 0.3628 - precision_m: 0.2743 - recall_m: 0.5469 - val_loss: 0.6932 - val_acc: 0.5000 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 9/10\n",
      "3125/3125 [==============================] - 341s 109ms/step - loss: 0.6932 - acc: 0.5004 - f1_m: 0.3114 - precision_m: 0.2352 - recall_m: 0.4701 - val_loss: 0.6931 - val_acc: 0.5000 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 10/10\n",
      "3125/3125 [==============================] - 335s 107ms/step - loss: 0.6932 - acc: 0.4994 - f1_m: 0.3234 - precision_m: 0.2442 - recall_m: 0.4890 - val_loss: 0.6931 - val_acc: 0.5000 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "cnn_history = cnn.fit(train_generator, epochs=10,validation_data=valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "  File \"C:\\Users\\sanya\\AppData\\Local\\Temp\\ipykernel_6360\\1408623295.py\", line 4, in <module>\n",
      "    training_accuracy = cnn_history.history['accuracy']\n",
      "KeyError: 'accuracy'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\pygments\\styles\\__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1114, in get_records\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\pygments\\styles\\__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "# Visualizing results\n",
    "training_loss = cnn_history.history['loss']\n",
    "validation_loss = cnn_history.history['val_loss']\n",
    "training_accuracy = cnn_history.history['accuracy']\n",
    "validation_accuracy = cnn_history.history['val_accuracy']\n",
    "training_precision = cnn_history.history['precision_m']\n",
    "validation_precision = cnn_history.history['val_precision_m']\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Plotting Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, training_loss, label='Training loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, training_precision, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_precision, label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Precision\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, training_precision, label='Training precision')\n",
    "plt.plot(epochs, validation_precision, label='Validation precision')\n",
    "plt.title('Precision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 202s 324ms/step - loss: 0.6931 - acc: 0.5000 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6931484341621399, 0.5, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing set\n",
    "cnn.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 754ms/step\n",
      "FAKE IMAGE\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Real image\n",
    "img_path = \"manual_test_images/Real.jpg\"\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "# plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 128, 128, 3))\n",
    "prediction = cnn.predict(img_array)\n",
    "if prediction[0][0] == 1:\n",
    "    print(\"REAL IMAGE\")\n",
    "else:\n",
    "    print(\"FAKE IMAGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "FAKE IMAGE\n"
     ]
    }
   ],
   "source": [
    "# Fake image(hard)\n",
    "img_path = \"manual_test_images/Hard.jpeg\"\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "# plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 128, 128, 3))\n",
    "prediction = cnn.predict(img_array)\n",
    "if prediction[0][0] == 1:\n",
    "    print(\"REAL IMAGE\")\n",
    "else:\n",
    "    print(\"FAKE IMAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "FAKE IMAGE\n"
     ]
    }
   ],
   "source": [
    "# Fake image(medium)\n",
    "img_path = \"manual_test_images/Medium.jpg\"\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "# plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 128, 128, 3))\n",
    "prediction = cnn.predict(img_array)\n",
    "if prediction[0][0] == 1:\n",
    "    print(\"REAL IMAGE\")\n",
    "else:\n",
    "    print(\"FAKE IMAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "  File \"C:\\Users\\sanya\\AppData\\Local\\Temp\\ipykernel_6360\\3922879377.py\", line 4, in <module>\n",
      "    plt.imshow(img)\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 3343, in imshow\n",
      "    __ret = gca().imshow(\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 2525, in gca\n",
      "    return gcf().gca()\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 1000, in gcf\n",
      "    return figure()\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 934, in figure\n",
      "    manager = new_figure_manager(\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 464, in new_figure_manager\n",
      "    _warn_if_gui_out_of_main_thread()\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 441, in _warn_if_gui_out_of_main_thread\n",
      "    canvas_class = cast(type[FigureCanvasBase], _get_backend_mod().FigureCanvas)\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 280, in _get_backend_mod\n",
      "    switch_backend(rcParams._get(\"backend\"))  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\matplotlib\\pyplot.py\", line 342, in switch_backend\n",
      "    module = importlib.import_module(cbook._backend_module_name(newbackend))\n",
      "  File \"C:\\Users\\sanya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'matplotlib_inline'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\pygments\\styles\\__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1114, in get_records\n",
      "  File \"c:\\Users\\sanya\\Documents\\AI image detection research\\generated-image-detection\\.env\\lib\\site-packages\\pygments\\styles\\__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "# Fake image(easy)\n",
    "img_path = \"manual_test_images/Easy.jpg\"\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "plt.imshow(img)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = preprocess_input(img_array.reshape(1, 128, 128, 3))\n",
    "prediction = cnn.predict(img_array)\n",
    "if prediction[0][0] == 1:\n",
    "    print(\"REAL IMAGE\")\n",
    "else:\n",
    "    print(\"FAKE IMAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "from datetime import datetime\n",
    "filename = f'models/cnn_{datetime.now().timestamp()}.h5'\n",
    "tf.keras.models.save_model(cnn, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
