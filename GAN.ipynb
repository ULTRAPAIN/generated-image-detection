{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kge2AWRM2ChE"
      },
      "source": [
        "# GAN creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjozlxQZ1lrF"
      },
      "outputs": [],
      "source": [
        "# For COLAB\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# !pip install -q kaggle\n",
        "# !mkdir ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets download xhlulu/140k-real-and-fake-faces\n",
        "# ! unzip /content/140k-real-and-fake-faces.zip\n",
        "# ! pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Sh8PurTu1pP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX6fDSB5u1pU",
        "outputId": "d894964e-1b5a-46c5-b80f-2fb5587d9f3e"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GXMi1ycu1pV",
        "outputId": "8799a847-f714-4ce3-cf0f-c431e40a1916"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt6xH9_ou1pW"
      },
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27np0gNROdY_"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKrrQ_DIu1pY"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "image_height = 64\n",
        "image_width = 64\n",
        "latent_features = 100\n",
        "num_images = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n8dQrZVOf5D"
      },
      "outputs": [],
      "source": [
        "# Define the input and output directories\n",
        "input_directory = 'real_vs_fake/real-vs-fake/test/'\n",
        "\n",
        "# Create a dataset from the images\n",
        "real_folder = os.path.join(input_directory, 'real')\n",
        "image_paths = [os.path.join(real_folder, filename) for filename in os.listdir(real_folder)[:num_images]]\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [image_height, image_width])\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = (img - 127.5) / 127.5\n",
        "    return img\n",
        "\n",
        "# Create a dataset from the image paths\n",
        "dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "LRtCtqvZu1pZ",
        "outputId": "c09055cc-4b7d-4241-a749-26fe1cb0916d"
      },
      "outputs": [],
      "source": [
        "for batch in dataset.take(1):  # Take the first batch for display\n",
        "    batch = (batch + 1) / 2\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(min(BATCH_SIZE, 9)):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(batch[i].numpy())\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUaj8beEmh--",
        "outputId": "bdb4c2e0-e70f-4d57-9ad8-53bc0ec4003a"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TJuQ0u2rtO"
      },
      "source": [
        "## GAN architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9eDGwSC2tg7"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, Conv2DTranspose, BatchNormalization, ZeroPadding2D, UpSampling2D, Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uen8TWNR2yXa",
        "outputId": "3b6f2c71-0cf7-49fe-a3ee-b89cc09bc134"
      },
      "outputs": [],
      "source": [
        "def build_generator():\n",
        "    init = RandomNormal(stddev=0.02, seed=69)\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4*4*512, input_dim=latent_features, kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Reshape((4,4,512)))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256, 5, 1,padding='same', kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, 5, 1,padding='same', kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, 5, 1,padding='same', kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(32, 5, 1,padding='same', kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    model.add(Conv2D(3, 5, 1,padding='same', activation='tanh', kernel_initializer=init))\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "apYqdZtF3FIT",
        "outputId": "fd176fad-0831-4f2c-e846-a7a328913d3a"
      },
      "outputs": [],
      "source": [
        "img = generator.predict(np.random.normal(size=(1, latent_features, 1)))\n",
        "plt.imshow(img[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F-nEObe3X5j",
        "outputId": "82b703ec-6c59-46d8-f313-54fccd9086c2"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "    init = RandomNormal(stddev=0.02, seed=69)\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(128, 3, 2, padding='same', input_shape = (image_width,image_height,3), kernel_initializer=init))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(256, 3, 2, padding='same', kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(512, 3, 2, padding='same', kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(1024, 3, 1, padding='same', kernel_initializer=init))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xtmXQdd3mMi",
        "outputId": "2d713640-4f53-4621-d4f4-7c36df1c4c7b"
      },
      "outputs": [],
      "source": [
        "img = generator.predict(np.random.normal(size=(4, latent_features, 1)))\n",
        "discriminator.predict(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXaiomkD33M1"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCo1UusGeibV"
      },
      "outputs": [],
      "source": [
        "# from keras.models import load_model\n",
        "# generator = load_model('models\\generator.h5')\n",
        "# discriminator = load_model('models\\discriminator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3WXmHfZ35Qh"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras.utils import array_to_img\n",
        "from keras.callbacks import Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88o1jvCc4GOe"
      },
      "outputs": [],
      "source": [
        "g_opt = Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "d_opt = Adam(learning_rate=0.0002,beta_1=0.5)\n",
        "g_loss = BinaryCrossentropy()\n",
        "d_loss = BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz5MSHRB4Ozf"
      },
      "outputs": [],
      "source": [
        "class GAN(Model):\n",
        "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
        "        # Pass through args and kwargs to base class\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Create attributes for gen and disc\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs):\n",
        "        # Compile with base class\n",
        "        super().compile(*args, **kwargs)\n",
        "\n",
        "        # Create attributes for losses and optimizers\n",
        "        self.g_opt = g_opt\n",
        "        self.d_opt = d_opt\n",
        "        self.g_loss = g_loss\n",
        "        self.d_loss = d_loss\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            real_images = batch\n",
        "            ypred_real = self.discriminator(real_images, training=True)\n",
        "            y_real = tf.zeros_like(ypred_real)\n",
        "            noise_real = 0.10*tf.random.uniform(tf.shape(ypred_real))\n",
        "            y_real += noise_real\n",
        "            real_d_loss = self.d_loss(y_real, ypred_real)\n",
        "\n",
        "            fake_images = self.generator(tf.random.normal((BATCH_SIZE, latent_features, 1)), training=False)\n",
        "            ypred_fake = self.discriminator(fake_images, training=True)\n",
        "            y_fake = tf.ones_like(ypred_fake)\n",
        "            noise_fake = -0.10*tf.random.uniform(tf.shape(ypred_fake))\n",
        "            y_fake += noise_fake\n",
        "            fake_d_loss = self.d_loss(y_fake, ypred_fake)\n",
        "\n",
        "            total_d_loss = real_d_loss + fake_d_loss\n",
        "        # Apply backpropagation - nn learn\n",
        "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
        "\n",
        "        # Train the generator\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            gen_images = self.generator(tf.random.normal((BATCH_SIZE,latent_features,1)), training=True)\n",
        "            predicted_labels = self.discriminator(gen_images, training=False)\n",
        "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels)\n",
        "        # Apply backprop\n",
        "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
        "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJR3HTsr4SQj"
      },
      "outputs": [],
      "source": [
        "gan = GAN(generator, discriminator)\n",
        "gan.compile(g_opt, d_opt, g_loss, d_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZgr9jxY4X-S"
      },
      "outputs": [],
      "source": [
        "class ModelMonitor(Callback):\n",
        "    def __init__(self, latent_dim=latent_features):\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 10 == 0:\n",
        "            random_latent_vectors = tf.random.uniform((1, self.latent_dim,1))\n",
        "            generated_images = self.model.generator(random_latent_vectors)\n",
        "            generated_images = (generated_images + 1) / 2\n",
        "            generated_images = generated_images.numpy()\n",
        "            img = array_to_img(generated_images[0])\n",
        "            os.makedirs(\"/content/training/\", exist_ok=True)\n",
        "            img.save(f'/content/training/{epoch}_generated_img.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY9e10pb4daj"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi8VqtIS4e36",
        "outputId": "2ed04060-f2b2-47a2-caa6-253de6617fa5"
      },
      "outputs": [],
      "source": [
        "hist = gan.fit(dataset, epochs=100, callbacks=[ModelMonitor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "64CgMB764imz",
        "outputId": "1aac0897-1c6c-4ff8-aa83-f79e145bffea"
      },
      "outputs": [],
      "source": [
        "plt.suptitle('Loss')\n",
        "plt.plot(hist.history['d_loss'], label='d_loss')\n",
        "plt.plot(hist.history['g_loss'], label='g_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKTYzDn245sY"
      },
      "outputs": [],
      "source": [
        "# generator.save('models/generator.h5')\n",
        "# discriminator.save('models/discriminator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHLZ3Uur4lDl"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Ml1OAwJT4mVC",
        "outputId": "51c5ef4f-7a30-4f00-cf39-ed19bf535f4f"
      },
      "outputs": [],
      "source": [
        "img = generator.predict(tf.random.normal((1, latent_features, 1)))\n",
        "img = (img + 1) / 2\n",
        "plt.imshow(img[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX25vK1viGqm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
